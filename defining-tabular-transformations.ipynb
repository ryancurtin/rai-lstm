{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0185aa",
   "metadata": {},
   "source": [
    "# Predicting a Tabular Transformation from Text\n",
    "We plan on solving data transformation with a translation type approach.  There are two facets here:\n",
    "1. Providing a better interface for data transformation\n",
    "2. _Predicting_ which transformations are needed to get to a given schema\n",
    "\n",
    "In the below notebook, I'm going to show a neural network that takes user-generated text as an input and predicts which transformation to use.  \n",
    "\n",
    "**To be clear**, this is a proof of concept, and the UI is a work in progress.  We plan on training against a massive corpus of text and identifying features within the input to determine which columns to operate on.  Given the following user input:\n",
    "\n",
    "```\n",
    "Concatenate the first_name and last_name columns into a new column called full_name\n",
    "```\n",
    "\n",
    "Our new model would produce a sequence of transformations with the named columns as inputs, automatically.\n",
    "\n",
    "While we're working out the kinks, a more basic demonstration will have to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d09a03a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "add_column_text = [\"add a column\",\"insert a column\",\"place new column\",\"add row numbers\",\"insert row numbers\"]\n",
    "remove_column_text = [\"remove column\",\"delete column\", \"remove row numbers\"]\n",
    "\n",
    "augmented_add_column = []\n",
    "for txt in add_column_text:\n",
    "  for num in range(9):\n",
    "    augmented_add_column.append(f'{txt} after column {num + 1}')\n",
    "    augmented_add_column.append(f'{txt} before column {num + 1}')\n",
    "\n",
    "augmented_remove_column = []\n",
    "for txt in remove_column_text:\n",
    "  for num in range(9):\n",
    "    augmented_remove_column.append(f'{txt} after column {num + 1}')\n",
    "    augmented_remove_column.append(f'{txt} before column {num + 1}')\n",
    "\n",
    "dict = {}\n",
    "dict['<unknown>'] = 0\n",
    "dict['a'] = 1\n",
    "dict['add'] = 2\n",
    "dict['column'] = 3\n",
    "dict['delete'] = 4\n",
    "dict['insert'] = 5\n",
    "dict['new'] = 6\n",
    "dict['numbers'] = 7\n",
    "dict['place'] = 8\n",
    "dict['remove'] = 9\n",
    "dict['row'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d83845d",
   "metadata": {},
   "source": [
    "### Training / Test Data\n",
    "We're processing user inputs into a six-word sequence.  We've augmented the data with text referencing where to perform the operation for the purposes of having a larger corpus of text.  These arguments won't be consumed in our demo for the sake of brevity.  The only feature engineering I'm going to perform is to encode our text as a one-hot vector with the limited vocabulary we've defined in the dictionary above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fc781c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_one_hot(text, indices):\n",
    "  text_arr = text.split()\n",
    "  text_arr += [''] * (seq_length - len(text_arr))\n",
    "  arr = np.zeros((seq_length, len(indices)))\n",
    "  for idx, token in enumerate(text_arr):\n",
    "    if token in indices:\n",
    "      oh_idx = indices[token]\n",
    "    else:\n",
    "      oh_idx = 0\n",
    "    arr[idx][oh_idx] = 1.0\n",
    "  return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5827e2c",
   "metadata": {},
   "source": [
    "Next, we're going to construct X and y vectors for use in training and testing our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a0b92728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations selected:\n",
    "# [add_column, remove_column, unknown]\n",
    "add_column_classification = np.array([1, 0, 0])\n",
    "remove_column_classification = np.array([0, 1, 0])\n",
    "\n",
    "add_column_y = np.full((len(augmented_add_column), 3), add_column_classification)\n",
    "remove_column_y = np.full((len(augmented_remove_column), 3), remove_column_classification)\n",
    "y = np.concatenate((add_column_y, remove_column_y), axis=0)\n",
    "\n",
    "inputs = []\n",
    "for text in augmented_add_column:\n",
    "  input = text_to_one_hot(text, dict)\n",
    "  inputs.append(input)\n",
    "\n",
    "for text in augmented_remove_column:\n",
    "  input = text_to_one_hot(text, dict)\n",
    "  inputs.append(input)\n",
    "\n",
    "X = np.array(inputs)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa952f6",
   "metadata": {},
   "source": [
    "### LSTM Model\n",
    "In order to classify this sequence of text, we're going to use a really basic LSTM model with a softmax output layer. The model will identify if the user wants to add a column or remove a column to this tabular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "34aed773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_33 (LSTM)              (None, 4)                 256       \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 3)                 15        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271\n",
      "Trainable params: 271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "115/115 [==============================] - 1s 2ms/step - loss: 1.0623 - accuracy: 0.4783\n",
      "Epoch 2/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.9099 - accuracy: 0.6174\n",
      "Epoch 3/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.7792 - accuracy: 0.6174\n",
      "Epoch 4/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6706 - accuracy: 0.6261\n",
      "Epoch 5/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.4241 - accuracy: 0.9478\n",
      "Epoch 6/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 1.0000\n",
      "Epoch 7/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 1.0000\n",
      "Epoch 8/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 1.0000\n",
      "Epoch 9/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0555 - accuracy: 1.0000\n",
      "Epoch 10/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0436 - accuracy: 1.0000\n",
      "Epoch 11/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0354 - accuracy: 1.0000\n",
      "Epoch 12/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0295 - accuracy: 1.0000\n",
      "Epoch 13/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 1.0000\n",
      "Epoch 14/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 15/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 16/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 17/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 18/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 19/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 20/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 21/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 22/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 23/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 24/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 25/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 26/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 27/32\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 28/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 29/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 30/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 31/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 32/32\n",
      "115/115 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x167350f10>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 6\n",
    "num_classifications = 3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(seq_length, len(dict))))\n",
    "model.add(Dense(num_classifications, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=1, epochs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3ef92c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af6529d",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "As you can see from this contrived example, we are 100% accurate and our work here is done.\n",
    "\n",
    "Ok, it's not going to be that easy, but this is the start of a really intuitive interface for users building tabular data transformations.  \n",
    "\n",
    "Aside from drastically improving the sophistication of this model, the next frontier is going to be interpreting data schemas and producing a **sequence of transformations** to translate existing data into the new schema.  Creating this automatic bridge would give us the ability for systems to exchange data with minimal user input.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rai-demo': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a9560e666882bed0e3a5f60ae52484e747a8d75fef3c477acb7cd1fa6e091b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
